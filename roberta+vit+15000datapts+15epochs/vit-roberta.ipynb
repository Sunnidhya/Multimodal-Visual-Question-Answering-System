{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#this code loads the first 200 examples from the validation split of the \n#\"HuggingFaceM4/VQAv2\" dataset into the variable named dataset.\n\nfrom datasets import load_dataset\n\n# dataset = load_dataset(\"HuggingFaceM4/VQAv2\")\ndataset = load_dataset(\"HuggingFaceM4/VQAv2\", split=[\"train[:25%]\", \"validation[:25%]\"])\ndataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Access the train split\ntrain_dataset = dataset[0]\n\n# Print the first row\nprint(train_dataset[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset[0]['image']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Access the validation split\nvalidation_dataset = dataset[1]\n\n# Print the first row\nprint(validation_dataset[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"validation_dataset[0]['image']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing image","metadata":{}},{"cell_type":"code","source":"#Read only the Answer space from this model (labels and the config file)\nfrom transformers import ViltConfig\nconfig = ViltConfig.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(config.id2label)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(validation_dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm.notebook import tqdm\n\ndef get_score(count: int) -> float:\n    return min(1.0, count / 3)\n\ndef add_labels_scores(annotation):\n\n    if(annotation['answers'] != None):\n        answers = annotation['answers']\n        answer_count = {}\n        for answer in answers:\n            answer_ = answer[\"answer\"]\n            answer_count[answer_] = answer_count.get(answer_, 0) + 1\n        labels = []\n        scores = []\n        for answer in answer_count:\n            if answer not in config.label2id:\n                continue\n            labels.append(config.label2id[answer])\n            score = get_score(answer_count[answer])\n            scores.append(score)\n        annotation['labels'] = labels\n        annotation['scores'] = scores\n \n    return annotation\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\nimport numpy as np\nfrom IPython.display import display\n\n\n#This is not the subsetting, so we take the whole train, the subsetting happens way below\nnum_samples_to_display = len(train_dataset)\nsubset_train = train_dataset.select(range(num_samples_to_display))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\nimport numpy as np\nfrom IPython.display import display\n\n#This is not the subsetting, so we take the whole validation, the subsetting happens way below\nnum_samples_to_display = len(validation_dataset)\nsubset_val = validation_dataset.select(range(num_samples_to_display))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def showImage(istrain=True, id=None):\n    if istrain:\n        data = subset_train\n    else:\n        data = subset_val\n    if id == None:\n        id = np.random.randint(len(data))\n    \n    modified_item = add_labels_scores(data[id])\n    #print(f\"Sample {id}: {modified_item}\\n\")\n    image = modified_item['image']\n\n    print(image)\n    display(image)\n\n    print(\"Question:\\t\", modified_item[\"question\"])\n    print(\"Answer:\\t\", modified_item[\"answers\"])\n    print(\"Labels:\\t\", modified_item[\"labels\"])\n    print(\"Scores:\\t\", modified_item[\"scores\"])\n    print(\"Scores for these labels:\\t\",[config.id2label[label] for label in modified_item[\"labels\"]])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"showImage(True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"showImage(False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom PIL import Image\n\nclass VQADataset(torch.utils.data.Dataset):\n    \"\"\"VQA (v2) dataset.\"\"\"\n\n    def __init__(self, questions, annotations, preprocessor,tokenizer):\n        self.questions = questions\n        self.annotations = annotations\n        self.preprocessor = preprocessor\n        self.tokenizer=tokenizer\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def __getitem__(self, idx):\n        # get image + text\n        annotations = self.annotations[idx]\n        questions = self.questions[idx]\n        image = annotations['image']\n        image = image.convert(\"RGB\")  # Explicitly convert the PIL Image object to RGB mode        \n        image = np.array(image)\n        text = questions['question']\n        \n        encoding = self.preprocessor(image, return_tensors=\"pt\")\n        encoded_text = self.tokenizer(\n            text=text,\n            padding='max_length',\n            max_length=24,\n            truncation=True,\n            return_tensors='pt',\n            return_token_type_ids=True,\n            return_attention_mask=True,\n        )\n\n        encoding [\"input_ids\"]= encoded_text['input_ids']\n        encoding [\"token_type_ids\"]= encoded_text['token_type_ids']\n        encoding [\"attention_mask\"]= encoded_text['attention_mask']\n\n\n        # remove batch dimension\n        for k,v in encoding.items():\n          encoding[k] = v.squeeze()\n        # add labels\n        labels = annotations['labels']\n        scores = annotations['scores']\n        # based on: https://github.com/dandelin/ViLT/blob/762fd3975c180db6fc88f577cf39549983fa373a/vilt/modules/objectives.py#L301\n        targets = torch.zeros(len(config.id2label))\n        for label, score in zip(labels, scores):\n              targets[label] = score\n        encoding[\"labels\"] = targets\n\n        return encoding\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Subsetting","metadata":{}},{"cell_type":"code","source":"import random\n\n# Specify the number of samples you want to use\nnum_samples = 15000\n\n# Randomly sample indices for our subset\nindices = random.sample(range(len(subset_train)), num_samples)\n\n# Create subset from the sampled indices\nsubset_questions = [{'question': subset_train[i]['question']} for i in indices]\nsubset_annotations = [add_labels_scores(subset_train[i]) for i in indices]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Specify the number of samples you want to use\nnum_samples = 3000\n\n# Randomly sample indices for our subset\nval_indices = random.sample(range(len(subset_val)), num_samples)\nsubset_val_questions = [{'question': subset_val[i]['question']} for i in val_indices]\nsubset_val_annotations = [add_labels_scores(subset_val[i]) for i in val_indices]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(subset_questions[0])\nprint(subset_annotations[0])\n     ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(subset_val_questions[0])\nprint(subset_val_annotations[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer,AutoFeatureExtractor\ntext='roberta-base'\nimage='google/vit-base-patch16-224-in21k'\ntokenizer = AutoTokenizer.from_pretrained(text)\npreprocessor=AutoFeatureExtractor.from_pretrained(image)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vqa2_dataset = VQADataset(questions=subset_questions,\n                     annotations=subset_annotations,\n                     preprocessor=preprocessor,\n                     tokenizer=tokenizer)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vqa2_dataset_val = VQADataset(questions=subset_val_questions,\n                     annotations=subset_val_annotations,\n                     preprocessor=preprocessor,\n                     tokenizer=tokenizer)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vqa2_dataset[0].keys()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vqa2_dataset_val[0].keys()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ndef collate_fn(batch):\n  input_ids = [item['input_ids'] for item in batch]\n  #[print(len(item)) for item in input_ids]\n  pixel_values = [item['pixel_values'] for item in batch]\n  attention_mask = [item['attention_mask'] for item in batch]\n  token_type_ids = [item['token_type_ids'] for item in batch]\n  labels = [item['labels'] for item in batch]\n  \n  # # create padded pixel values and corresponding pixel mask\n  # encoding = processor.feature_extractor.pad_and_create_pixel_mask(pixel_values, return_tensors=\"pt\")\n  \n  # create new batch\n  batch = {}\n  batch['pixel_values'] = torch.stack(pixel_values)\n  batch['input_ids'] = torch.stack(input_ids)\n  batch['token_type_ids'] = torch.stack(token_type_ids)\n  batch['attention_mask'] = torch.stack(attention_mask)\n  # batch['pixel_mask'] = encoding['pixel_mask']\n  batch['labels'] = torch.stack(labels)\n  \n  return batch\n\ntrain_dataloader = DataLoader(vqa2_dataset, collate_fn=collate_fn, batch_size=4, shuffle=True,num_workers=4)\nval_dataloader = DataLoader(vqa2_dataset_val, collate_fn=collate_fn, batch_size=4,num_workers=4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch = next(iter(train_dataloader))\nfor k,v in batch.items():\n  print(k, v.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## PyTorch\nimport torch\nimport torch.nn as nn\nimport torch.utils.data as data\nimport torch.optim as optim\n\nfrom typing import Dict, List, Optional, Tuple","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define the Model","metadata":{}},{"cell_type":"code","source":"class MultimodalVQAModel(nn.Module):\n    def __init__(\n            self,\n            num_labels: int = len(config.id2label),\n            intermediate_dim: int = 512,\n            pretrained_text_name: str = 'roberta-base',\n            pretrained_image_name: str = 'google/vit-base-patch16-224-in21k'):\n     \n        super(MultimodalVQAModel, self).__init__()\n        self.num_labels = num_labels\n        self.pretrained_text_name = pretrained_text_name\n        self.pretrained_image_name = pretrained_image_name\n        \n        self.text_encoder = AutoModel.from_pretrained(\n            self.pretrained_text_name,\n        )\n        self.image_encoder = AutoModel.from_pretrained(\n            self.pretrained_image_name,\n        )\n        self.fusion = nn.Sequential(\n            nn.Linear(self.text_encoder.config.hidden_size + self.image_encoder.config.hidden_size, intermediate_dim),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n        )\n        \n        self.classifier = nn.Linear(intermediate_dim, self.num_labels)\n        \n        self.criterion = nn.CrossEntropyLoss()\n    def forward(\n            self,\n            pixel_values: torch.FloatTensor,\n            input_ids: torch.LongTensor,\n            token_type_ids: Optional[torch.LongTensor] = None,\n            attention_mask: Optional[torch.LongTensor] = None,\n            labels: Optional[torch.LongTensor] = None):\n        \n        encoded_text = self.text_encoder(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            return_dict=True,\n        )\n        encoded_image = self.image_encoder(\n            pixel_values=pixel_values,\n            return_dict=True,\n        )\n        fused_output = self.fusion(\n            torch.cat(\n                [\n                    encoded_text['pooler_output'],\n                    encoded_image['pooler_output'],\n                ],\n                dim=1\n            )\n        )\n        logits = self.classifier(fused_output)\n        out = {\n            \"logits\": logits\n        }\n        if labels is not None:\n            loss = self.criterion(logits, labels)\n            out[\"loss\"] = loss\n        return out","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function for setting the seed\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\nset_seed(42)\n\n# Ensure that all operations are deterministic on GPU (if used) for reproducibility\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\ndevice = torch.device(\"cpu\") if not torch.cuda.is_available() else torch.device(\"cuda:0\")\nprint(\"Using device\", device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Structure","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModel","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = MultimodalVQAModel()\nmodel","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Forward Pass","metadata":{}},{"cell_type":"code","source":"example = vqa2_dataset[0]\nprint(example.keys())\n# add batch dimension + move to GPU\nexample = {k: v.unsqueeze(0).to(device) for k,v in example.items()}\nprint(example)\n# forward pass\nmodel.to(device)\noutputs = model(**example)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = np.random.randint(len(config.id2label), size=5)\npreds = np.random.randint(len(config.id2label), size=5)\n\ndef showAnswers(ids):\n    print([config.id2label[id] for id in ids])\n\nshowAnswers(labels)\nshowAnswers(preds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Number of parameters of the model","metadata":{}},{"cell_type":"code","source":"def countTrainableParameters(model):\n    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    print(\"No. of trainable parameters:\\t{0:,}\".format(num_params))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"countTrainableParameters(model) # For BERT-ViT model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Using Lightning module for training","metadata":{}},{"cell_type":"code","source":"!pip install pytorch-lightning","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n\nclass LightningMultimodalVQAModel(pl.LightningModule):\n    def __init__(self, model):\n        super().__init__()\n        self.model = model\n\n    def forward(self, pixel_values, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n        return self.model(pixel_values=pixel_values, input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask, labels=labels)\n\n    def training_step(self, batch, batch_idx):\n        output = self(**batch)\n        loss = output[\"loss\"]\n        self.log('train_loss', loss)\n        \n        # Compute additional metrics\n        preds = torch.argmax(output[\"logits\"], dim=1).detach().cpu().numpy()\n        labels = torch.argmax(batch[\"labels\"], dim=1).detach().cpu().numpy()\n\n        accuracy = accuracy_score(labels, preds)\n        precision = precision_score(labels, preds, average='weighted')\n        recall = recall_score(labels, preds, average='weighted')\n        f1 = f1_score(labels, preds, average='weighted')\n\n        self.log('train_accuracy', accuracy, on_step=True, on_epoch=True, prog_bar=True)\n        self.log('train_precision', precision, on_step=True, on_epoch=True, prog_bar=True)\n        self.log('train_recall', recall, on_step=True, on_epoch=True, prog_bar=True)\n        self.log('train_f1', f1, on_step=True, on_epoch=True, prog_bar=True)\n        \n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        output = self(**batch)\n        loss = output[\"loss\"]\n        self.log('val_loss', loss)\n\n        # Compute additional metrics\n        preds = torch.argmax(output[\"logits\"], dim=1).detach().cpu().numpy()\n        labels = torch.argmax(batch[\"labels\"], dim=1).detach().cpu().numpy()\n\n        accuracy = accuracy_score(labels, preds)\n        precision = precision_score(labels, preds, average='weighted')\n        recall = recall_score(labels, preds, average='weighted')\n        f1 = f1_score(labels, preds, average='weighted')\n\n        self.log('val_accuracy', accuracy, on_step=True, on_epoch=True, prog_bar=True)\n        self.log('val_precision', precision, on_step=True, on_epoch=True, prog_bar=True)\n        self.log('val_recall', recall, on_step=True, on_epoch=True, prog_bar=True)\n        self.log('val_f1', f1, on_step=True, on_epoch=True, prog_bar=True)\n\n    def test_step(self, batch, batch_idx):\n        output = self(**batch)\n        loss = output[\"loss\"]\n        self.log('test_loss', loss)\n\n        # Compute additional metrics\n        preds = torch.argmax(output[\"logits\"], dim=1).detach().cpu().numpy()\n        labels = torch.argmax(batch[\"labels\"], dim=1).detach().cpu().numpy()\n\n        accuracy = accuracy_score(labels, preds)\n        precision = precision_score(labels, preds, average='weighted')\n        recall = recall_score(labels, preds, average='weighted')\n        f1 = f1_score(labels, preds, average='weighted')\n\n        self.log('test_accuracy', accuracy, on_step=True, on_epoch=True, prog_bar=True)\n        self.log('test_precision', precision, on_step=True, on_epoch=True, prog_bar=True)\n        self.log('test_recall', recall, on_step=True, on_epoch=True, prog_bar=True)\n        self.log('test_f1', f1, on_step=True, on_epoch=True, prog_bar=True)\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.AdamW(self.model.parameters(), lr=5e-5)\n        return optimizer\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score,f1_score\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize your model\nmultimodal_model = model\nlightning_model = LightningMultimodalVQAModel(multimodal_model)\n\n# Define the checkpoint callback\ncheckpoint_callback = ModelCheckpoint(\n    dirpath=\"my_model/checkpoint/\",\n    save_top_k=1,  # Save only the best model\n    verbose=True,\n    monitor=\"val_accuracy\",\n    mode=\"max\"\n)\n\n# Initialize the trainer\ntrainer = pl.Trainer(max_epochs=15,  callbacks=[checkpoint_callback])\n\n# Fit the model\ntrainer.fit(lightning_model, train_dataloader, val_dataloader)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %load_ext tensorboard\n# %tensorboard --logdir lightning_logs/ --port 4000","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"# # Load the model from the checkpoint\n# checkpoint_path = \"/kaggle/working/my_model/checkpoint/epoch=0-step=1250.ckpt\"\n# lightning_model = LightningMultimodalVQAModel.load_from_checkpoint(checkpoint_path, model=MultimodalVQAModel())\n\n# # Place the model into evaluation mode and move it to the correct device\n# lightning_model = lightning_model.to(device)\n# lightning_model.eval()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# For Validation","metadata":{}},{"cell_type":"code","source":"# id=56\n# showImage(False,id)\n# example = vqa2_dataset_val[id]\n# example = {k: v.unsqueeze(0).to(device) for k,v in example.items()}\n\n# # Forward pass\n# with torch.no_grad():\n#     input_ids = example[\"input_ids\"]\n#     print(input_ids.shape)\n#     outputs = lightning_model(**example)\n#     logits = outputs[\"logits\"]\n#     top2_values, top2_indices = logits.topk(2, dim=-1)\n#     predicted_classes = top2_indices.squeeze().tolist()\n#     print(\"Predicted answers:\", [config.id2label[predicted_class] for predicted_class in predicted_classes])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# id=41\n# showImage(False,id)\n# example = vqa2_dataset_val[id]\n# example = {k: v.unsqueeze(0).to(device) for k,v in example.items()}\n\n# # Forward pass\n# with torch.no_grad():\n#     outputs = lightning_model(**example)\n#     logits = outputs[\"logits\"]\n#     top2_values, top2_indices = logits.topk(2, dim=-1)\n#     predicted_classes = top2_indices.squeeze().tolist()\n#     print(\"Predicted answers:\", [config.id2label[predicted_class] for predicted_class in predicted_classes])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# id=90\n# showImage(False,id)\n# example = vqa2_dataset_val[id]\n# example = {k: v.unsqueeze(0).to(device) for k,v in example.items()}\n# example\n\n# # Forward pass\n# with torch.no_grad():\n#     outputs = lightning_model(**example)\n#     logits = outputs[\"logits\"]\n#     top2_values, top2_indices = logits.topk(2, dim=-1)\n#     predicted_classes = top2_indices.squeeze().tolist()\n#     print(\"Predicted answers:\", [config.id2label[predicted_class] for predicted_class in predicted_classes])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# id=154\n# showImage(False,id)\n# example = vqa2_dataset_val[id]\n# example = {k: v.unsqueeze(0).to(device) for k,v in example.items()}\n\n# # Forward pass\n# with torch.no_grad():\n#     outputs = lightning_model(**example)\n#     logits = outputs[\"logits\"]\n#     top2_values, top2_indices = logits.topk(2, dim=-1)\n#     predicted_classes = top2_indices.squeeze().tolist()\n#     print(\"Predicted answers:\", [config.id2label[predicted_class] for predicted_class in predicted_classes])","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}