  <h3>Multimodal-Question-Answering-Model</h3>
  <hr>
  <h6>
  <ul>
    <li>BERT and ViT are indeed multimodal models that have been utilized in various applications, particularly in the field of multimodal deep learning.</li>
    <li> BERT, which stands for Bidirectional Encoder Representations from Transformers, is an encoder-only Transformer model that excels in natural language processing tasks 
         by predicting masked tokens based on context. On the other hand, ViT, or Vision Transformer, revolutionized computer vision tasks by treating images as fixed-size 
         patches and creating embeddings from them. ViT uses a standard Transformer encoder to process images, allowing for efficient handling of visual data without 
         convolutions.</li>
    <li>These multimodal models, by integrating the strengths of BERT and ViT, have significantly advanced the capabilities of machines to understand and interpret data from 
         different modalities, leading to breakthroughs in tasks such as visual question answering</li>
  </ul>
  <hr>
  <h4>To know more about our project just click on the pdf</h4>
  

</h6>
